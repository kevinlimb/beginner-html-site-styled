<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Introduction to Network Mathematics</title>
<style type="text/css">
<!--
body,td,th {
	font-family: Geneva, Arial, Helvetica, sans-serif;
}
body {
	background-color: #F0F0F0;
	margin-left: 1in;
	margin-right: 1in;
}
.style3 {font-size: smaller}
.style6 {color: #000000}
.style9 {font-size: xx-large}
.style10 {font-size: small}
.style11 {font-size: x-small}
.style12 {font-size: x-large}
-->
</style>
</head>

<body>
<h1 align="center"><strong>Introduction to Network Mathematics</strong></h1>
<p align="center"><strong>By Bruce Hoppe</strong></p>
<table border="2">
    <tr>
    <td bgcolor="#FF99CC"><strong>If your browser does not support mathematical symbols, you will not see this document correctly. If &quot;⊆&quot; looks like a box in quotes instead of a lazy U, or this <a title="Math symbol test" href="http://cs-people.bu.edu/behoppe/math%20symbol%20test.html">math-symbol-test-page</a> looks like a bunch of boxes, then you need to find a browser that supports mathematical symbols. In our experience, <a href="http://www.mozilla.com/en-US/firefox/">Firefox</a> works and Internet Explorer does not.</strong></td>
  </tr>
</table>
<p><em><strong>Introduction to Network Mathematics</strong></em><strong> provides college students with basic graph theory to better understand the Internet.</strong> <span class="style3">Many passages are edited from <a href="http://en.wikipedia.org/">Wikipedia</a>, a few are from <a href="http://planetmath.org/">PlanetMath</a>, and others are original writing by <a href="http://connectedness.blogspot.com/">Bruce Hoppe</a>, who teaches CS-103: &quot;Introduction to Internet Technologies and Web Programming&quot; at Boston University. This is a work in progress, first created in the spring semester of 2007, and now being used in the spring semester 2009.</span><br />
</p>
<p><a name="cr" id="cr"><strong>Copyright &copy; 2007, 2008, 2009 by Bruce Hoppe. </strong></a><span class="style3">Permission is granted to copy, distribute and/or modify these documents under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant<a name="copyright" id="copyright"></a> Sections, with no Front-Cover Texts, and with no Back-Cover Texts. A copy of the license is included in the section entitled <a href="http://aux.planetmath.org/fdl/fdl.txt">&quot;GNU Free Documentation License&quot;</a>. If you believe a portion of this site infringes on your or anyone's copyright, please <a href="mailto:behoppe@bu.edu">send us a note</a>.</span> </p>
<h1>Contents</h1>
<ul>
  <li><a href="#sets">Sets</a></li>
  <ul>
    <li><a href="#Specifying_sets">Explicit and implicit set notation</a></li>
    <li><a href="#cardinality">Cardinality</a></li>
    <li><a href="#Subsets">Subsets</a></li>
    <li><a href="#unions">Unions, intersections, and complements</a></li>
    <li><a href="#jaccard">Similarity of sets: Jaccard index</a></li>
    <li> <a href="#ordered lists">Ordered pairs and ordered lists</a></li>
  </ul>
  <li> <a href="#graph">Graphs</a></li>
  <ul>
    <li> <a href="#undirected_directed">Undirected and directed graphs</a></li>
    <li> <a href="#adjacent">Adjacent and Incident</a></li>
    <li><a href="#neighborhood">Neighborhood and Degree</a></li>
    <li> <a href="#density">Density and average degree</a></li>
    <li><a href="#paths">Paths and walks</a></li>
    <li><a href="#length">Length, distance, diameter</a></li>
    <li><a href="#subgraphs">Induced Subgraphs</a></li>
    <li><a href="#connected">Connected vertices, graphs, and components</a></li>
    <li><a href="#cluster">Components, cliques, and clusters</a></li>
    <li><a href="#struct">Structural equivalence</a></li>
  </ul>
  <li><a href="#network_structure">Network Structure</a><br />
  </li>
  <ul>
    <li><a href="#bipartite">Bipartite (affiliation) graphs</a></li>
    <li><a href="#bookmarks">Social bookmarking, bipartite graphs, and structural equivalence</a><br />
    </li>
    <li><a href="#trees">Trees and forests</a></li>
    <li>Minimum spanning trees</li>
    <li>Kruskal's algorithm<br />
    </li>
    <li><a href="#pagerank">PageRank</a></li>
  </ul>
</ul>
<hr/>
<h1><a name="sets" id="sets">Sets</a><br />
</h1>
<p>A <strong>set</strong> is a collection of objects. These objects are called the <strong>elements</strong> or <strong>members</strong> of the set. Objects can be anything: numbers, people, other sets, etc. For instance, 4 is a member of the set of all even <a title="Integer" href="http://en.wikipedia.org/wiki/Integer">integers</a>. Clearly, the set of even integers is infinitely large; there is no requirement that a set be finite.</p>
<p>If <em>x</em> is a member of <em>A</em>, then we write <em>x</em> <img alt="\in" src="http://upload.wikimedia.org/math/8/c/2/8c20c78b364ed5dbadd49e5b997aa1cc.png" /> <em>A</em>. We may also say that <em>x</em> &quot;belongs to&quot; <em>A</em>, or that <em>x</em> &quot;is in&quot; <em>A</em>. </p>
<p>If <em>x</em> is not a member of <em>A, </em>then we write <em>x</em> <img alt="\notin" src="http://upload.wikimedia.org/math/6/9/2/692614f72e231db0d2313050ac29e113.png" /> <em>A.</em></p>
<p>Two sets <em>A</em> and <em>B</em> are defined to be <strong><a title="Equality (mathematics)" href="http://en.wikipedia.org/wiki/Equality_%28mathematics%29">equal</a></strong> when they have precisely the same elements, that is, if every element of <em>A</em> is an element of <em>B</em> and every element of <em>B</em> is an element of <em>A</em>. Thus a set is completely determined by its elements; the description is immaterial. For example, the set with elements 2, 3, and 5 is equal to the set of all <a title="Prime number" href="http://en.wikipedia.org/wiki/Prime_number">prime numbers</a> less than 6. <br />
</p>
<p>If the sets <em>A</em> and <em>B</em> are equal, then we write <em>A</em> = <em>B</em>.<br />
</p>
<p>We also allow for an <strong><a title="Empty set" href="http://en.wikipedia.org/wiki/Empty_set">empty set</a></strong>, a set without any members at all. We write the empty set as { }.<br />
</p>
<p><a id="Specifying_sets" name="Specifying_sets"></a></p>
<h2> Explicit and Implicit Set Notation<br />
</h2>
<p>Explicit Set Notation: The simplest way to describe a set is to list its elements between curly braces, known as defining a set <em>explicitly</em>. Thus {1,2} denotes the set whose only elements are 1 and 2. Note the following points:</p>
<ul>
  <li>Order of elements is immaterial: for example, {1,2} = {2,1}.</li>
  <li>Repetition of elements is irrelevant: for example, {1,2,2} = {1,1,1,2} = {1,2}.</li>
</ul>
<p>Warning: This notation can be informally abused by saying something like {dogs} to indicate the set of all dogs, but this example would be read by mathematicians as &quot;the set containing the single element <em>dogs</em>&quot;.</p>
<p>The simplest possible example of explicit set notation is { }, which denotes the empty set.</p>
<p>Implicit Set Notation: We can also use the notation {<em>x</em> : <em>P</em>(<em>x</em>)} to denote the set containing all objects for which the condition <em>P</em> holds, known as defining a set <em>implicitly</em>. For example, {<em>x</em> : <em>x</em> is a real number} denotes the set of <a title="Real number" href="http://en.wikipedia.org/wiki/Real_number">real numbers</a>, {<em>x</em> : <em>x</em> has blonde hair} denotes the set of everything with blonde hair, and {<em>x</em> : <em>x</em> is a dog} denotes the set of all dogs.</p>
<h2><a name="cardinality" id="cardinality">Cardinality</a></h2>
Given a finite set A, the <a href="http://en.wikipedia.org/wiki/Cardinality">cardinality</a> of A is simply the number of elements in A. We write the cardinality of A as |A|, which we may read as &quot;cardinality of A&quot; or &quot;size of A.&quot; Note that this vertical-bar notation is the same as used to denote absolute value, but the meaning of cardinality is quite different from absolute value. In particular, absolute value operates on numbers (e.g., |-3| = 3) while cardinality operates on sets (e.g., |{-3}| = 1). Other examples of cardinality are:<br />
<ul>
  <li>|{3,4}| = 2.</li>
  <li>|{ }| = 0. The empty set has no elements.</li>
  <li>|{{1,2},{3,4}}| = 2. In this case the two elements of {{1,2},{3,4}} are themselves sets: {1,2} and {3,4}.</li>
  <li>|{x: x is registered for CS-103}| = the number of people registered for CS-103.</li>
</ul>
<p><a id="Subsets" name="Subsets"></a></p>
We can also consider the cardinality of an infinite set A, but that discussion is beyond the scope of this introduction.<br />
<h2> Subsets</h2>
<p>Given two sets <em>A</em> and <em>B</em> we say that <em>A</em> is a <strong><a title="Subset" href="http://en.wikipedia.org/wiki/Subset">subset</a></strong> of <em>B</em> if every element of <em>A</em> is also an element of <em>B</em>. Notice that in particular, <em>B</em> is a subset of itself; a subset of <em>B</em> that isn't equal to <em>B</em> is called a <strong>proper subset</strong>.</p>
<p>If <em>A</em> is a subset of <em>B</em>, then one can also say that <em>B</em> is a <strong>superset</strong> of <em>A</em>, that <em>A</em> is <strong>contained in</strong> <em>B</em>, or that <em>B</em> <strong>contains</strong> <em>A</em>. In symbols, <em>A</em> ⊆ <em>B</em> means that <em>A</em> is a subset of <em>B</em>, and <em>B</em> ⊇ <em>A</em> means that <em>B</em> is a superset of <em>A</em>. For clarity, we use &quot;⊆&quot; and &quot;⊇&quot; for subsets (which allow set equality) and we use &quot;⊂&quot; and &quot;⊃&quot; are reserved for proper subsets (which exclude equality).</p>
<p>For example, let A be the set of American citizens, B be the set of registered students at Boston University, and C be the set of registered students in CS-103. Then C is a proper subset of B and B contains C, which we can write as C ⊂ <em>B </em>and<em>B</em> ⊃ <em>C. </em>If we allow the possibility that every BU student might be registered for CS-103, then we would write C ⊆ <em>B or </em><em>B</em> ⊇ <em>C</em> to allow for this unlikely situation. Note that not all sets are comparable in this way. For example, it is not the case either that A is a subset of B nor that B is a subset of A.<br />
</p>
<h2> <a name="unions" id="unions">Unions</a>, intersections, and relative complements</h2>
<p>Given two sets <em>A</em> and <em>B</em>, the <strong><a title="Union (set theory)" href="http://en.wikipedia.org/wiki/Union_%28set_theory%29">union</a></strong> of A and B is the set consisting of all objects which are elements of <em>A</em> or of <em>B</em> or of both. It is written as<em> A</em> ∪ <em>B</em>.</p>
<p>The <strong><a title="Intersection (set theory)" href="http://en.wikipedia.org/wiki/Intersection_%28set_theory%29">intersection</a></strong> of <em>A</em> and <em>B</em> is the set of all objects which are both in <em>A</em> and in <em>B</em>. It is written as <em>A</em> ∩ <em>B</em>.</p>
<p>Finally, the <strong><a title="Complement (set theory)" href="http://en.wikipedia.org/wiki/Complement_%28set_theory%29">relative complement</a></strong> of <em>B</em> relative to <em>A</em>, also known as the <strong>set theoretic difference</strong> of <em>A</em> and <em>B</em>, is the set of all objects that belong to <em>A</em> but <em>not</em> to <em>B</em>. It is written as <em>A</em> \ <em>B</em>. Symbolically, these are respectively</p>
<dl>
  <dd><em>A</em> ∪ B := {<em>x</em> : (<em>x</em> <img alt="\in" src="http://upload.wikimedia.org/math/8/c/2/8c20c78b364ed5dbadd49e5b997aa1cc.png" /> <em>A</em>) <a title="Logical disjunction" href="http://en.wikipedia.org/wiki/Logical_disjunction">or</a> (<em>x</em> <img alt="\in" src="http://upload.wikimedia.org/math/8/c/2/8c20c78b364ed5dbadd49e5b997aa1cc.png" /> <em>B</em>)}</dd>
  <dd><em>A</em> ∩ <em>B</em> := {<em>x</em> : (<em>x</em> <img alt="\in" src="http://upload.wikimedia.org/math/8/c/2/8c20c78b364ed5dbadd49e5b997aa1cc.png" /> <em>A</em>) <a title="Logical conjunction" href="http://en.wikipedia.org/wiki/Logical_conjunction">and</a> (<em>x</em> <img alt="\in" src="http://upload.wikimedia.org/math/8/c/2/8c20c78b364ed5dbadd49e5b997aa1cc.png" /> <em>B</em>)}</dd>
  <dd><em>A</em> \ <em>B</em> := {<em>x</em> : (<em>x</em> <img alt="\in" src="http://upload.wikimedia.org/math/8/c/2/8c20c78b364ed5dbadd49e5b997aa1cc.png" /> <em>A</em>) and <a title="Negation" href="http://en.wikipedia.org/wiki/Negation">not</a> (<em>x</em> <img alt="\in" src="http://upload.wikimedia.org/math/8/c/2/8c20c78b364ed5dbadd49e5b997aa1cc.png" /> <em>B</em>) }</dd>
</dl>
<p>Notice that B doesn't have to be a subset of <em>A</em> for <em>A \ B</em> to make sense.<img src="images/Venn-Diagram.jpg" alt="Venn Diagram" width="331" height="279" hspace="20" vspace="20" border="2" align="right" /><br />
</p>
<p>To illustrate these ideas, let <em>A</em> be the set of left-handed people, and let <em>B</em> be the set of people with blond hair. Then <em>A</em> ∩ <em>B</em> is the set of all left-handed blond-haired people, while <em>A</em> ∪ <em>B</em> is the set of all people who are left-handed or blond-haired or both. <em>A</em> \ <em>B</em>, on the other hand, is the set of all people that are left-handed but not blond-haired, while <em>B</em> \ <em>A</em> is the set of all people that have blond hair but aren't left-handed.</p>
<p><strong>Venn diagrams</strong> are illustrations that show these relationships pictorally. The above example can be drawn as the Venn diagram at right:</p>
<p><strong>Disjoint sets:</strong> Let <em>E</em> be the set of all human beings, and let <em>F</em> be the set of all living things over 1000 years old. What is <em>E</em> ∩ <em>F</em> in this case? No human being is over 1000 years old, so <em>E</em> ∩ <em>F</em> must be the <a title="Empty set" href="http://en.wikipedia.org/wiki/Empty_set">empty set</a> { }. When the intersection of any two sets is empty, we say those two sets are disjoint.<br />
</p>
<h2><a name="jaccard" id="jaccard"></a>Similarity of sets: Jaccard index</h2>
<p>We  measure the similarity of two non-empty sets A and B with the <strong><a href="http://en.wikipedia.org/wiki/Jaccard_index">Jaccard Index</a></strong>: J(A,B) = |A ∩ B| / |A ∪ B|.</p>
<p>If A and B are disjoint then J(A,B)=0. If A=B then J(A,B)=1. (Assuming A and B are non-empty.)</p>
<p>Suppose A = {1,2,3,4,5,6}. Below we use J(A,B) to measure the similarity of set A with 5 different example sets B:</p>
<table width="0%" border="2" cellspacing="5" cellpadding="5">
  <tr>
    <td><strong>Set  B</strong></td>
    <td><strong>Venn diagram of sets A and B</strong></td>
    <td><strong>Jaccard index J(A,B)</strong></td>
  </tr>
  <tr>
    <td>B = {7,8,9,10,11,12}</td>
    <td><div align="center"><img src="images/jaccard-1.jpg" alt="jaccard-1" width="331" height="158" /></div></td>
    <td><p>J(A,B) = </p>
      <span class="style10">|{1,2,3,4,5,6} ∩ {7,8,9,10,11,12}|</span><br/>
      <hr/><span class="style10">|{1,2,3,4,5,6} ∪ {7,8,9,10,11,12}|</span>
      <p>= 0/12;</p>
      <p>Not at all similar</p></td>
  </tr>
  <tr>
    <td>B = {4,5,6,7,8,9}</td>
    <td><div align="center"><img src="images/jaccard-2.jpg" alt="j-2" width="240" height="158" /></div></td>
    <td><p>J(A,B) =</p>
      <p align="center"><span class="style10">|{1,2,3,4,5,6} ∩ {4,5,6,7,8,9}|</span><br/>
</p>
      <hr align="center"/>
      <div align="center"><span class="style10">|{1,2,3,4,5,6} ∪ {4,5,6,7,8,9}|</span>
      </div>
      <p>= 3/9;</p>
      <p>Somewhat similar</p></td>
  </tr>
  <tr>
    <td>B = {2,3,4,5,6,7}</td>
    <td><div align="center"><img src="images/jaccard-3.jpg" alt="j-3" width="169" height="158" /></div></td>
    <td><p>J(A,B) = 5/7</p>
      <p>Quite similar</p></td>
  </tr>
  <tr>
    <td>B = {4}</td>
    <td><div align="center"><img src="images/jaccard-4.jpg" alt="j-4" width="154" height="158" /></div></td>
    <td><p>J(A,B) = 1/6</p>    </td>
  </tr>
  <tr>
    <td>B = {4,5,6,7, ... 25}</td>
    <td><img src="images/jaccard-5.jpg" alt="j-5" width="331" height="274" /></td>
    <td>J(A,B) = 3/25</td>
  </tr>
</table>
<h2><a name="ordered lists" id="ordered lists2">Ordered </a>Pairs and Ordered Lists</h2>
<p>Intuitively, an <strong><a title="Ordered pair" href="http://en.wikipedia.org/wiki/Ordered_pair">ordered pair</a></strong> is simply a collection of two objects such that one can be distinguished as the <em>first element</em> and the other as the <em>second element</em>, and having the fundamental property that two ordered pairs are equal if and only if their <em>first elements</em> are equal and their <em>second elements</em> are equal.</p>
<p>We write an ordered pair using parentheses--not curly braces, which are used for sets. For example, the ordered pair (a,b) consists of first element <em>a</em> and second element <em>b</em>.</p>
<p>Two ordered pairs (<em>a</em>,<em>b</em>) and (<em>c</em>,<em>d</em>) are equal if and only if <em>a</em> = <em>c</em> and <em>b</em> = <em>d</em>.</p>
In contrast, a set of two elements is an unordered pair which we write using curly braces and where there is no distinction between the first element and the second element. It follows that<br />
<ul>
  <li>Two ordered pairs (<em>a</em>,<em>b</em>) and (<em>c</em>,<em>d</em>) are equal if and only if <em>a</em> = <em>c</em> and <em>b</em> = <em>d</em></li>
  <li>Two sets {a,b} and {c,d} are equal if and only if <br />
  </li>
  <ol>
    <li>Either a = c and b = d<br />
    </li>
    <li>Or a = d and b = c</li>
  </ol>
</ul>
We can also write ordered lists having any finite number of elements with an intuitive extension of the above notation. For example:<br />
<ul>
  <li>(1,2,3) is an ordered list. Note that (1,2,3) is different from (3,2,1).</li>
  <li>{1,2,3} is a set. Note that {1,2,3} = {3,2,1}.</li>
  <li>{(1,2),(3,4)} is a set of two elements, each of which is an ordered pair. One of the elements of the set is (1,2) and the other element of the set is (3,4).</li>
  <li>({1,2},{3},{2,4}) is an ordered list of sets. The first element of the list is the set {1,2}; the second element of the list is the set {3}; the third element of the list is the set {2,4}.</li>
</ul>
<h1><a name="graph" id="graph">Graphs</a></h1>
Mathematicians have developed <img src="images/six nodes.jpg" alt="Six-Node Graph" hspace="20" vspace="20" border="2" align="right" title="Six-Node Graph" />  graph theory in order to study all kinds of networks. A graph   is a set of objects connected by lines.
<p>The objects in a graph are usually called <em>nodes</em> or <em>vertices. </em>The lines connecting the objects are usually called <em>links</em> or <em>edges</em>. <br />
</p>
<p>More formally, we define a graph G as an <a title="Ordered pair" href="http://en.wikipedia.org/wiki/Ordered_pair">ordered pair</a> <em>G</em> = (<em>V</em>,<em>E</em>) where <br />
</p>
<ul>
  <li><em>V</em> is a <a title="Set" href="http://en.wikipedia.org/wiki/Set">set</a> of vertices (nodes)</li>
  <li><em>E</em> is a set of edges (links).</li>
  <li>Each edge is a pair of vertices. In other words, each element of E is a pair of elements of V.</li>
</ul>
<p>Example: The picture above represents the following graph:</p>
<ul>
  <li><em>V</em> = {1,2,3,4,5,6}</li>
  <li><em>E</em> = {{1,2},{1,5},{2,3},{2,5},{3,4},{4,5},{4,6}}</li>
</ul>
Loops: An edge with identical endpoints is a loop: <img src="images/loop.gif" alt="loop" title="loop" border="0" hspace="0" vspace="0" /> Unless explicitly stated otherwise, we disallow loops.<br />
<br />
Trivial and empty graphs: The graph with only one vertex and no edges is called the trivial graph. The graph with no vertices and no edges is called the empty graph.
<h2><a name="undirected_directed" id="undirected_directed">Undirected </a>and Directed Graphs </h2>
<h2> </h2>
<p><strong>Undirected graph:</strong>  The edges of a graph are assumed to be unordered pairs of vertices. Sometimes we say <em>undirected</em> graph to  emphasize this point. In an undirected graph, we write edges using curly braces to denote unordered pairs. For example, an undirected edge {2,3} from vertex 2 to vertex 3 is the same thing as an undirected edge {3,2} from vertex 3 to vertex 2. <br />
</p>
<p><strong>Directed graph:</strong>  In a <em>directed </em>graph, the two directions are counted as being distinct <em>directed edges</em>. In an directed graph, we write edges using parentheses to denote ordered pairs. For example, edge (2,3) is directed from 2 to 3 , which is different than the directed edge (3,2) from 3 to 2. Directed graphs are drawn with arrowheads on the links, as shown below:<br /><img src="images/three-node networks.jpg" alt="Directed and Undirected" width="638" height="263" hspace="20" vspace="20" border="2" align="absmiddle" /> </p>
<h2><a name="adjacent" id="adjacent">Adjacent </a>and Incident</h2>
<p>Two vertices are called <strong>adjacent</strong> if they share a common edge, in which case the common edge is said to <strong>join</strong> the two vertices. An edge and a vertex on that edge are called <strong>incident</strong>. </p>
<p>See the 6-node graph below right for examples of <strong>adjacent</strong> and <strong>incident</strong>: </p>
<ul>
  <li>Nodes 4 and 6 are adjacent (as well as many other pairs of nodes)</li>
  <li>Nodes 1 and 3 are not adjacent (as well as many other pairs of nodes)</li>
  <li>Edge {2,5} is incident to node 2 and node 5.</li>
</ul>
<h2><a name="neighborhood" id="neighborhood">Neighborhood and Degree</a></h2>
<p>The <strong>neighborhood</strong> of a vertex <em>v</em> in a graph <em>G</em> is the set of vertices adjacent to v. The neighborhood is denoted <em>N</em>(<em>v</em>). The neighborhood does not include <em>v</em> itself. For example, in the graph below N(5) = {4,2,1} and N(6) = {4}.</p>
<p>The <strong>degree</strong> of a vertex is the total number of vertices adjacent to the vertex. The degree of a vertex <em>v</em> is denoted deg(<em>v</em>). We can equivalently define the degree of a vertex as the cardinality of its neighborhood and say that for any vertex <em>v</em>, deg(<em>v</em>) = <em>|N</em>(<em>v</em>)|.<img src="images/six nodes.jpg" alt="Six Nodes" width="311" height="200" hspace="20" vspace="20" border="2" align="right" /></p>
<p>The  <a title="Undirected graph" href="http://en.wikipedia.org/wiki/Undirected_graph">undirected graph</a>  at right has the following degrees:</p>
<table border="2">
  <tbody>
    <tr>
      <th>Vertex </th>
      <th>Degree </th>
    </tr>
    <tr>
      <td>1 </td>
      <td>2 </td>
    </tr>
    <tr>
      <td>2 </td>
      <td>3 </td>
    </tr>
    <tr>
      <td>3 </td>
      <td>2 </td>
    </tr>
    <tr>
      <td>4 </td>
      <td>3 </td>
    </tr>
    <tr>
      <td>5 </td>
      <td>3 </td>
    </tr>
    <tr>
      <td>6 </td>
      <td>1 </td>
    </tr>
  </tbody>
</table>
<p>In a <a title="Directed graph" href="http://en.wikipedia.org/wiki/Directed_graph">directed graph</a>, we define degree exactly the same as above (and note that &quot;adjacent&quot; does not imply any direction or lack of direction). It is also important to define <strong>indegree</strong> and <strong>outdegree</strong>. Recall that any directed edge has two distinct ends: a head (the end with an arrowhead) and a tail. Each end is counted separately. The sum of head endpoints count toward the <strong>indegree</strong> of a vertex and the sum of tail endpoints count toward the <strong>outdegree</strong> of a vertex. The directed graph at right has the following degrees, indegrees, and outdegrees:<img src="images/Directed_graph.svg" alt="Directed Graph" width="180" height="119" hspace="20" vspace="20" border="2" align="right" /></p>
<table border="2">
  <tbody>
    <tr>
      <th>Vertex </th>
      <th>Degree</th>
      <th>Indegree </th>
      <th>Outdegree </th>
    </tr>
    <tr>
      <td>1 </td>
      <td>2</td>
      <td>0 </td>
      <td>2 </td>
    </tr>
    <tr>
      <td>2 </td>
      <td>2</td>
      <td>2 </td>
      <td>0 </td>
    </tr>
    <tr>
      <td>3 </td>
      <td>3</td>
      <td>2 </td>
      <td>2 </td>
    </tr>
    <tr>
      <td>4 </td>
      <td>1</td>
      <td>1 </td>
      <td>1 </td>
    </tr>
  </tbody>
</table>
<p>Pay particular attention to nodes 3 and 4 in the above table. If they seem confusing, try re-drawing the above graph using the &quot;easier way to draw&quot; illustrated <a href="#undirected_directed">previously</a>.<br />
</p>
<h2><a name="density" id="density">Density </a>and Average Degree</h2>
<p>The <strong> density</strong> of a graph G = (V,E) measures how many edges are in set E compared to the maximum possible number of edges between vertices in set V. Density is calculated as follows:<br />
</p>
<ul>
  <li>An undirected graph with no loops can have at most |<em>V</em>| * (|<em>V</em>| − 1) / 2 edges, so the density of an undirected graph is 2 * |E| / (|<em>V</em>| * (|<em>V</em>| − 1)).</li>
  <li>A <a title="Directed graph" href="http://en.wikipedia.org/wiki/Directed_graph">directed graph</a> with no loops can have at most |<em>V</em>| * (|<em>V</em>| − 1) edges, so the density of a directed graph is |E| / (|<em>V</em>| * (|<em>V</em>| − 1))</li>
</ul>
The average degree of a graph G is another measure of how many edges are in set E compared to number of vertices in set V. Because each edge is incident to two vertices and counts in the degree of both vertices, the average degree of a graph is 2*|E|/|V|
<h2><a name="paths" id="paths">Paths </a>and Walks<br />
</h2>
<p>A <strong>path</strong> P = (v<sub>1</sub>,v<sub>2</sub>,v<sub>3</sub>,...,v<sub>k</sub>) is an ordered list of <a title="Vertex (graph theory)" href="http://en.wikipedia.org/wiki/Vertex_%28graph_theory%29">vertices</a> such that<br />
</p>
<ol>
  <li>From each of its vertices there is an <a title="Edge (graph theory)" href="http://en.wikipedia.org/wiki/Edge_%28graph_theory%29">edge</a> to the next vertex in the list<br />
  </li>
  <li>No vertex occurs in the list more than once. </li>
</ol>
<p align="left">The first vertex of a path is called the <em>origin</em> and the last vertex is called the <em>destination</em>. Both origin and destination are called <em>endpoints </em>of the path.</p>
<p align="left">Example: In the graph below, there are many paths from node 6 to node 1. One such path is P<sub>1</sub> = (6, 4, 3, 2, 5, 1); another path from 6 to 1 is P<sub>2</sub> = (6, 4, 5, 1).</p>
<p align="left">Any node all by itself makes a trivial path. For example, (3) is the path that begins and ends at node 3. Another way to see (3) is as an ordered list with exactly one element: the node 3.</p>
<p align="center"><img src="images/six nodes.jpg" alt="Six Nodes" width="311" hspace="20" vspace="20" border="2" /><br />
</p>
<p>We say <strong>walk</strong> to denote an ordered list of vertices that follows the first requirement of a path but not the second. In other words, a walk may visit the same vertex more than once, but a path must never do so. This implies that every path is a walk, but some walks are not paths (i.e., any walk that visits a vertex more than once). A <strong><a title="Cycle graph" href="http://en.wikipedia.org/wiki/Cycle_graph">cycle</a></strong> is a walk such that the origin and destination are the same; a <strong>simple cycle</strong> is a cycle that does not repeat any vertices other than the origin and destination.</p>
<p>Examples: In the graph above, there are many walks from node 6 to node 1. Our previous examples P<sub>1</sub> = (6, 4, 3, 2, 5, 1) and P<sub>2</sub> = (6, 4, 5, 1) are also walks from node 6 to node 1. In addition, W<sub>1</sub> = (6,4,3,4,3,5,4,3,2,5,1) and W<sub>2</sub> = (6,4,5,1,5,1) are examples of walks from node 6 to to node 1 that are not paths.<br />
</p>
<p>The definitions of paths and walks can easily be extended to apply  to <a title="Graph (mathematics)" href="http://en.wikipedia.org/wiki/Graph_%28mathematics%29#Directed_graph">directed graphs</a>, in which case we use the terms <em>directed path</em>, <em>directed walk</em>, and <em>directed cycle</em>.<br />
</p>
<h2><a name="length" id="length2">Length</a>, Distance, Diameter<br />
</h2>
<p align="left">The <strong>length</strong> of a path or walk is the number of edges that it uses, counting multiple edges multiple times. In the graph below, (5, 2, 1) is a path of length 2 and (1, 2, 5, 1, 2, 3) is a walk of length 5. Also, the trivial path (3) has length 0.</p>
<p align="center"><img src="images/six nodes.jpg" alt="Six Nodes" width="311" hspace="20" vspace="20" border="2" /><br />
</p>
<p>The <strong>distance</strong> between two vertices x and y is written d(x,y) and defined as the length of the shortest path from x to y. The distance between any node and itself is 0. If there is no path from x to y (i.e., if x and y are in different connected components), then d(x,y) is infinity.<br />
</p>
<p>The <strong>average distance</strong> of a graph G is the average distance between all distinct pairs of vertices in G.</p>
<p>The <strong>diameter</strong> of a graph G is the maximum distance   between any pair of vertices  in G. If G is not connected, then the diameter of G is infinity.</p>
<p align="left">Example 1: consider the graph G=(V,E) drawn below.</p>
<p align="center"><img src="images/5-node ring.png" alt="5-node ring" width="130" border="2" /></p>
<p align="left">With a graph like the above example, diameter is similar to our geometric notion -- i.e., how far to get from one side of the circle to the other. In a graph context, we want to find the pair of nodes x and y such that the distance between x and y is not exceeded by the distance separating any other pair of nodes in the graph. One way to think about this is by filling in the table below, which shows the distance between each pair of nodes in G.</p>
<table width="0%" border="2" align="center" cellpadding="2" cellspacing="2">
  <tr>
    <td><strong>Nodes</strong></td>
    <td><strong>1</strong></td>
    <td><strong>2</strong></td>
    <td><strong>3</strong></td>
    <td><strong>4</strong></td>
    <td><strong>5</strong></td>
  </tr>
  <tr>
    <td><strong>1</strong></td>
    <td>0</td>
    <td>1</td>
    <td>2</td>
    <td>2</td>
    <td>1</td>
  </tr>
  <tr>
    <td><strong>2</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
    <td>1</td>
    <td>2</td>
    <td>2</td>
  </tr>
  <tr>
    <td><strong>3</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
    <td>1</td>
    <td>2</td>
  </tr>
  <tr>
    <td><strong>4</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><strong>5</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
  </tr>
</table>
<p>Because G is an undirected graph, we know that the distance from x to y is the same as the distance from y to x, and so we can ignore the shaded cells in the above table for the purposes of computing diameter and average distance.</p>
<p>After computing the distance between every possible pair of nodes, we see that the maximum distance in the above graph is 2. Therefore the diameter of the graph is 2.</p>
<p>The average distance of the graph is the average of all the non-zero cells in the above table. There are 10 such cells and the sum of the numbers in those cells is 15, making the average distance of G = 15/10 = 1.5.</p>
<p>Example 2: consider the graph G=(V,E) drawn below.</p>
<p align="center"><img src="images/diameter.jpg" alt="diamter" height="178" border="2" /></p>
<p align="left">The table below shows the distance between each pair of nodes in G.</p>
<table width="0%" border="2" align="center" cellpadding="2" cellspacing="2">
  <tr>
    <td><strong>Nodes</strong></td>
    <td><strong>1</strong></td>
    <td><strong>2</strong></td>
    <td><strong>3</strong></td>
    <td><strong>4</strong></td>
    <td><strong>5</strong></td>
    <td><strong>6</strong></td>
    <td><strong>7</strong></td>
    <td><strong>8</strong></td>
    <td><strong>9</strong></td>
    <td><strong>10</strong></td>
  </tr>
  <tr>
    <td><strong>1</strong></td>
    <td>0</td>
    <td>2</td>
    <td>3</td>
    <td>3</td>
    <td>3</td>
    <td>2</td>
    <td>4</td>
    <td>1</td>
    <td>2</td>
    <td>1</td>
  </tr>
  <tr>
    <td><strong>2</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
    <td>3</td>
    <td>3</td>
    <td>3</td>
    <td>2</td>
    <td>4</td>
    <td>3</td>
    <td>2</td>
    <td>1</td>
  </tr>
  <tr>
    <td><strong>3</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
    <td>2</td>
    <td>4</td>
    <td>1</td>
    <td>3</td>
    <td>4</td>
    <td>3</td>
    <td>2</td>
  </tr>
  <tr>
    <td><strong>4</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
    <td>2</td>
    <td>1</td>
    <td>1</td>
    <td>4</td>
    <td>3</td>
    <td>2</td>
  </tr>
  <tr>
    <td><strong>5</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
    <td>3</td>
    <td>1</td>
    <td>4</td>
    <td>1</td>
    <td>2</td>
  </tr>
  <tr>
    <td><strong>6</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
    <td>2</td>
    <td>3</td>
    <td>2</td>
    <td>1</td>
  </tr>
  <tr>
    <td><strong>7</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
    <td>5</td>
    <td>2</td>
    <td>3</td>
  </tr>
  <tr>
    <td><strong>8</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
    <td>3</td>
    <td>2</td>
  </tr>
  <tr>
    <td><strong>9</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><strong>10</strong></td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td bgcolor="#666666">&nbsp;</td>
    <td>0</td>
  </tr>
</table>
<p align="left">The average distance of G is the average of all the non-zero cells in the above table. There are 45 such cells and the sum of the numbers in those cells is 164, making the average distance of G = 164/45 = 3.6444.</p>
<p align="left">The diameter of G is the maximum value in any single cell in the above table. One cell stores value 5--because the distance from node 7 to node 8 is 5. There is no value larger than 5 in any other cell--no pair of nodes in G has distance more than 5. Therefore the diameter of G = 5.</p>
<h2><a name="subgraphs" id="subgraphs"></a>Induced Subgraphs</h2>
<p>It is often useful to consider only part of a graph. <strong>Induced subgraphs</strong> are one particularly convenient way to define a specific sub-part of a larger graph.</p>
<table border="2" align="right" cellpadding="0" cellspacing="0">
  <tr>
    <td><img src="images/induced-subgraph.gif" alt="Induced Subgraph" width="341" height="204" /><br />
      Graph G = (V,E)</td>
  </tr>
  <tr>
    <td><img src="images/induced-subgraph2.jpg" alt="Induced Subgraph" width="193" height="127" /><br />
      Subgraph induced by subset of nodes {1,2,3,4}</td>
  </tr>
</table>
<p> Given a graph G=(V,E), an induced subgraph is defined by a subset of vertices S ⊆V. Then the nodes of the  subgraph induced by S are simply the nodes in S, and the edges of the subgraph induced by S are all edges with both endpoints in S.</p>
<p>The upper graph pictured at right  shows G=(V,E) with V={x: x is an integer from 1 to 8} and edges as drawn. </p>
<p>The lower  picture shows the subgraph induced by the subset of nodes S={1, 2, 3, 4}. Notice that this induced subgraph consists of two connected components, even though the graph as a whole is a single connected component.</p>
<p>Another Example: In lecture we have discussed the Facebook friends network. We can write this as a graph using implicit notation as follows: G = (V,E), where V = {x: x is a person on Facebook} and E = {{x,y}: x <img alt="\in" src="http://upload.wikimedia.org/math/8/c/2/8c20c78b364ed5dbadd49e5b997aa1cc.png" /> V and y <img alt="\in" src="http://upload.wikimedia.org/math/8/c/2/8c20c78b364ed5dbadd49e5b997aa1cc.png" /> V and (x and y are friends on Facebook)}. The Facebook friends network is enormous, and so graph G is impossible to draw by hand; however, we can choose a small subset S of nodes in V and draw the graph induced by that subset S. This is exactly what the Touchgraph Facebook browser does: It lets you choose set S to be some number of your &quot;Top Friends&quot; on Facebook, possibly with the addition of yourself. Given this set S, Touchgraph displays a  subgraph of the entire Facebook friends network: specifically, the subgraph induced by your chosen number of &quot;Top Friends&quot; (possibly with the addition of yourself).</p>
<p>Below is the subgraph of the Facebook friends network induced by the subset S={Patti, Jenny, Bill, Deborah}. This set S is exactly Bruce Hoppe's &quot;Top 4 Friends&quot; (which does not include Bruce Hoppe). The edges are hard to see but they are {{Patti, Jenny}, {Jenny, Bill}, {Bill, Patti}, {Bill, Deborah}}. There are no edges joining Deborah and Patti, or Deborah and Jenny, because these people are not Facebook friends.</p>
<p><img width="100%" src="http://cs-people.bu.edu/behoppe/videos/facebook-friends.JPG" alt="Subgraph by Touchgraph" /></p>
<h2><a name="connected" id="connected">Connected vertices, graphs, and c</a>omponents</h2>
<p>Given an undirected graph, two vertices are <strong>connected</strong> if there is at least one path that joins them. For example, in the graph below, nodes 2 and 4 are connected. Nodes 1 and 6 are not connected.</p>
<p><strong>Connected vs. Adjacent:</strong> Any two nodes that are adjacent are also connected (e.g., nodes 2 and 5 below). Two nodes that are connected may not be adjacent (e.g., nodes 2 and 4 below). </p>
<p>Furthermore, any node is connected to itself and is not adjacent to itself. For example, node 2 is connected to node 2 by the path (2)--i.e., the zero-length path that consists of nothing but the node 2. Node 2 is not adjacent to node 2 because there is no edge {2,2} (i.e., no edge {2}); and our prohibition against loops in graphs implies that no node can ever be adjacent to itself.</p>
<p align="center"><img src="images/6-node 2-component.png" alt="6-node 2-component" width="254" height="181" border="2" /></p>
<p>A <strong>connected graph</strong> is one where each vertex can reach every other vertex by one or more paths.<a id="tex2html2" href="http://planetmath.org/encyclopedia/OpenWalk.html" name="tex2html2"></a> A connected graph is said to consist of a single connected component. If a graph is not connected, then it consists of two or more connected components. The graph above consists of two connected components. Below is another example.<br />
</p>
<p align="center"><img src="images/connected components.jpg" alt="Connected Components" width="532" height="191" /><br />
</p>
<p>Given an undirected graph G=(V,E), we define a <strong>connected component</strong> to be a subgraph induced by node set S<strong>⊆</strong>V (i.e., G<sub>S</sub> = (S,E<sub>S</sub>)) with the  following two properties:<br />
</p>
<ol>
  <li>G<sub>S</sub> is connected</li>
  <li>There is no edge in E that joins a node in S to a node not in S</li>
</ol>
<p><em>Six Degrees</em> describes connected components metaphorically using buttons as nodes and threads as edges. In this metaphor, a connected component is what you get when you lift one button off the floor--up comes not only that one button but also all the threads tied to that button, all the other buttons tied to those threads, etc. The above 2 mathematical properties of a connected component translate into this metaphor as follows:</p>
<ol>
  <li>G<sub>S</sub> is connected: The only buttons that rise off the floor do so because of the one button you are lifting and the threads that ultimately connect that button to other buttons (i.e., paths)</li>
  <li>There is no edge in E that joins a node in S to a node not in S: You have lifted the one button in your hand high enough so that no more buttons will come off the floor no matter how much higher you lift your hand.<img src="images/6-node 2-component.png" alt="6-node 2-component" width="127" hspace="5" vspace="5" border="2" align="right" /></li>
</ol>
<p>For example, the graph G=(V,E) drawn at right is not a connected component because it violates property #1: it is not connected.</p>
<p>The subgraph induced by S={3,4,5,6} (drawn with dark nodes and edges below as  G<sub>S</sub> = ({3,4,5,6},{{3,4},{3,5},{3,6},{4,5},{4,6},{5,6}}) is not a connected component because it violates property #2. There are edges in E that join nodes in S to node 2, which is a node not in S.<img src="images/subgraph 3456.png" alt="subgraph" width="127" hspace="5" vspace="5" border="2" align="right" /></p>
<p>&nbsp;</p>
<h2>&nbsp;</h2>
<h2><a name="cluster" id="cluster"></a>Components, Cliques, and Clusters</h2>
<p>Clusters are important features of the Web. To the extent that Web builders tend to link to sites similar to their own sites, clusters of interlinked Web pages represent collections of content about meaningful subtopics. </p>
<p>Clusters are hard to define mathematically. We will define clusters somewhat informally, by using precise definitions of connected components and cliques as stepping stones to a less precise definition of clusters.</p>
<p>In the preceding section we defined a connected component in an undirected graph G=(V,E) to be a subgraph induced by node set S<strong>⊆</strong>V (i.e., G<sub>S</sub> = (S,E<sub>S</sub>)) with the  following two properties:<br />
</p>
<ol>
  <li>G<sub>S</sub> is connected</li>
  <li>There is no edge in E that joins a node in S to a node not in S</li>
</ol>
<p>To that definition we add the notion of clique. In an undirected graph G=(V,E) a <strong>clique</strong> is a subgraph induced  by node set S<strong>⊆</strong>V (i.e., G<sub>S</sub> = (S,E<sub>S</sub>)) such that the density of G<sub>S</sub> is 1 (i.e., every pair of nodes in S is adjacent). Notice that the definition of clique does not depend on how many edges in E join nodes in S to nodes not in S.</p>
<p>Example: In the graph G drawn above, the subgraph induced by S={3,4,5,6} is a clique. Also, the subgraph induced by {2,6} is a (somewhat less interesting) clique. The subgraph induced by S={3,4,5,6} is the largest clique in G. Usually the largest clique in a graph is the most interesting clique in that graph.</p>
<p>The notion of cluster informally combines elements of our definitions of connected components and cliques. In an undirected graph G=(V,E) a <strong>cluster</strong> is a subgraph induced by node set S<strong>⊆</strong>V (i.e., G<sub>S</sub> = (S,E<sub>S</sub>)) with the  following two properties:<br />
</p>
<ol>
  <li>The density of G<sub>S</sub> is &quot;relatively high&quot;; (a relaxed adaptation of clique-ness)</li>
  <li>There are &quot;relatively few&quot; edges in E that join a node in S to a node not in S; (a relaxed adaptation of connected component-ness)</li>
</ol>
<p>Example: In the graph drawn below, the following subsets of nodes induce subgraphs that can fairly be called clusters (given our informal definition):<img src="images/components clusters cliques.png" alt="clusters" width="304" hspace="5" vspace="5" border="2" align="right" /></p>
<ul>
  <li>{1,2,3,4,5,7}</li>
  <li>{20,21,22,23,24,25}</li>
  <li>{14,15,16}</li>
</ul>
<p align="left">Not all connected components are clusters. In the graph drawn at right, even the subgraph induced by S={9,10,11,12,13} is arguably not a cluster, because the density of that induced subgraph is relatively low.</p>
<p align="left">Not all cliques are clusters--even relatively large cliques. The subgraph induced by S={1,2,3,4} is not a cluster because every single one of those nodes is adjacent to node 5; there are too many edges joining nodes in S to nodes not in S. Even the largest clique in the above graph--the subgraph induced by S={1,2,3,4,5}--is arguably still not a cluster because node 7 is adjacent to so many nodes in S.</p>
<h2><a name="struct" id="struct"></a>Structural equivalence</h2>
<p align="left">Finding clusters is  one way to discover  meaningful collections of Web pages. Another way is based on the idea that  Web pages x and y are similar to the extent that they have similar network neighborhoods -- even if x and y are not adjacent to each other. This is the idea behind structural equivalence.</p>
<p align="left">Consider G=(V,E) drawn below. (Edge set E includes both black and grey edges; we will get to that distinction momentarily.) Suppose G is a subgraph of the Facebook friends network and you are logged in as node 26. Which node will Facebook suggest first as someone you may know? Facebook will not bother to suggest someone you already know; instead, it will look for someone who is not yet a Facebook friend of yours but who has many of the same Facebook friends that you have. For example, node 29 stands out as &quot;someone you may know&quot; if you are node 26. By comparison, node 27 appears to be someone that node 26 probably does not know, judging by the different sets of friends that nodes 26 and 27 keep. In the middle ground, nodes 28, 30, and 31 each have some neighbors in common with node 26 and other neighbors that are distinct from those of node 26. Structural equivalence is a metric that can be used to rank these nodes according to how similar their neighborhoods are to that of node 26.</p>
<p align="center"><img src="images/struct-equiv3.png" alt="se" width="700" border="2" /></p>
<p>Given two nodes x and y in a graph, the <a href="#jaccard">Jaccard index</a> provides a simple and useful way to compare the similarity of their neighborhoods. Recall that the Jaccard index measures the similarity of two sets and is defined J(A,B) = |A ∩ B| / |A ∪ B|.</p>
<p>We define the <strong>structural equivalence</strong> of two nodes x and y as the similarity of their neighborhoods, as measured by the Jaccard index:</p>
<p align="center">SE(x,y) = J(N(x),N(y)) = |N(x) ∩ N(y)| / |N(x) ∪ N(y)|.</p>
<p class="style6">This equals 1 when x and y have identical neighborhoods and 0 when the neighborhoods of x and y are disjoint.</p>
<p class="style6">Consider graph G drawn above and suppose x=26; then N(6) = {1,2,3,4,5,6}. Below we compute the structural equivalence of  x with 5 example nodes y: 27, 28, 29, 30, and 31. </p>
<table width="0%" border="2" align="center" cellpadding="5" cellspacing="5">
  <tr>
    <td><strong>Node y</strong></td>
    <td><strong>N(y) = Neighborhood of y</strong></td>
    <td><strong>SE(x,y) = J(N(x),N(y))</strong></td>
  </tr>
  <tr>
    <td>27</td>
    <td><div align="center">{7,8,9,10,11,12}</div></td>
    <td><p>J(N(x),N(y)) = </p>
        <span class="style10">|{1,2,3,4,5,6} ∩ {7,8,9,10,11,12}|</span><br/>
        <hr/>
      <span class="style10">|{1,2,3,4,5,6} ∪ {7,8,9,10,11,12}|</span>
        <p>= 0/12; Not at all similar</p>
    </td>
  </tr>
  <tr>
    <td>28</td>
    <td><div align="center">{4,5,6,7,8,9}</div></td>
    <td><p>J(N(x),N(y)) =</p>
        <p align="center"><span class="style10">|{1,2,3,4,5,6} ∩ {4,5,6,7,8,9}|</span><br/>
        </p>
      <hr align="center"/>
        <div align="center"><span class="style10">|{1,2,3,4,5,6} ∪ {4,5,6,7,8,9}|</span> </div>
      <p>= 3/9; Somewhat similar</p>
    </td>
  </tr>
  <tr>
    <td>29</td>
    <td><div align="center">{2,3,4,5,6,7}</div></td>
    <td><p>J(N(x),N(y)) = 5/7; Quite similar</p>
    </td>
  </tr>
  <tr>
    <td>30</td>
    <td><div align="center">{4}</div></td>
    <td><p>J(N(x),N(y)) = 1/6</p></td>
  </tr>
  <tr>
    <td>31</td>
    <td><div align="center">{4,5,6,7, ... , 25}</div></td>
    <td>J(N(x),N(y)) = 3/25</td>
  </tr>
</table>
<p>To emphasize the underlying congruence between measuring the similarity of nodes using structural equivalence and the similarity of sets using the Jaccard index, we have arranged the above examples of structural equivalence to correspond exactly to the table of examples illustrating the <a href="#jaccard">Jaccard index</a>.<br />
</p>
<p>Structural equivalence, social bookmarking, and collaborative filtering: By far the most important Web application of structural equivalence is its use by sites such as Delicious and Amazon. When you look up a specific tag on Delicious and it recommends related tags, that recommendation is based on structural equivalence. When you shop on Amazon and it recommends other items you might also like to buy, that recommendation is based on structural equivalence. Websites that work in this way are often said to use <a href="http://pespmc1.vub.ac.be/collfilt.html">collaborative filtering</a>.</p>
<p>Our above definition of structural equivalence works the same in both the Facebook &quot;someone you may know&quot; examples and the Delicious/Amazon &quot;other things you may be interested in&quot; collaborative filtering scenario. However, the underlying graph used in the Delicious/Amazon scenario is a special kind of graph, often called an affiliation network or a bipartite graph.</p>
<p>We define bipartite graphs formally in the next section. For now, we simply re-draw the above example graph without any of the grey edges. The new graph is G'=(V,E'):</p>
<p align="center"><img src="images/struct-equiv4.png" alt="se4" width="700" border="2" /></p>
<p align="left">The graph above not only omits the grey edges of E, but also distinguishes nodes by two types. In a Delicious example, the red squares would be tags, the blue circles would be Web pages, and the links would represent bookmarks on Delicious that associate a tag with a Web page. In an Amazon example, the red squares would be books, the blue circles would be Amazon users, and the links would represent purchases.</p>
<p align="left">Unlike Facebook, where any node can invite any other node to be friends, Delicious and Amazon do not much care (relatively speaking) about any edges other than those between red squares and blue circles. By eliminating edges that join blue circles to blue circles, and edges that join red squares to red squares, we get a  graph that yields to much simpler algorithms while still modeling the connections that are most important to the task at hand (i.e., recommending new items of interest).</p>
<hr />
<h1><a name="network_structure" id="network_structure">Network Structure</a></h1>
<h2><a name="bipartite" id="bipartite">Bipartite </a>(Affiliation) Graphs</h2>
<h2> </h2>
A <strong>bipartite graph</strong> is as a graph whose nodes can each be <a title="Graph coloring" href="http://en.wikipedia.org/wiki/Graph_coloring">colored</a> red or blue such that there are no red-red edges and no blue-blue edges. The following graph, for example, is bipartite and so can be colored red and blue without creating red-red edges or blue-blue edges:
<p align="center">
  <!-- MATH  \begin{displaymath} \begin{xy} *!C\xybox{ \xymatrix{ {\color{red}A} \ar@{-}[rrr] \ar@{-}[rd] \ar@{-}[ddd] & & & {\color{blue}B} \ & {\color{blue}E} & {\color{red}F} \ar@{-}[ur] \ar@{-}[l] \ar@{-}[d] & \ & {\color{red}H} \ar@{-}[u] \ar@{-}[r] \ar@{-}[dl] & {\color{blue}G} & \ {\color{blue}D} & & & {\color{red}C} \ar@{-}[uuu] \ar@{-}[lll] \ar@{-}[ul] } } \end{xy} \end{displaymath}  -->
  <img src="images/red-blue net.png" alt="Red-Blue Net" width="220" height="223" hspace="20" vspace="20" border="2" align="middle" /></p>
<div align="center"></div>
<p>Another way to think of a bipartite graph is by drawing the red nodes on one side of the picture and the blue nodes on the other side of the picture:<br />
</p>
<p align="center">
  <!-- MATH  \begin{displaymath} \begin{xy} *!C\xybox{ \xymatrix{ {\color{red}A} \ar@{-}[r] \ar@{-}[rd] \ar@{-}[rdd] & {\color{blue}E} \ {\color{red}F} \ar@{-}[ru] \ar@{-}[r] \ar@{-}[rdd] & {\color{blue}B} \ {\color{red}H} \ar@{-}[ruu] \ar@{-}[r] \ar@{-}[rd] & {\color{blue}D} \ {\color{red}C} \ar@{-}[ruu] \ar@{-}[ru] \ar@{-}[r] & {\color{blue}G}} } \end{xy} \end{displaymath}  -->
  <img src="images/red-blue net 2-columns.png" alt="Red-Blue 2-Columns" width="95" height="223" hspace="20" vspace="20" border="2" align="middle" /></p>
<div align="center"></div>
Bipartite graphs are very commonly drawn in this way, with one subset of vertices on one side and the other subset on the opposite side. For the graph to be bipartite, edges may join vertices across columns <strong>but may never join vertices in the same column.</strong><br />
<br />
Bipartite graphs are useful for modelling <a title="Matching problem" href="http://en.wikipedia.org/wiki/Matching_problem">matching problems</a>. For example, which actors have worked on which a href="https://www.name-generators.org/movie-tv-game/">movies</a>? Which websites are bookmarked by which people? These are bipartite graphs.<br />
<br />
Most graphs are non-bipartite. A graph is non-bipartite when there is:<br />
<ol><li>No way to color its vertices red and blue without creating red-red or blue-blue edges</li>
  <li>No way to draw its vertices in two columns so that edges only cross from column to column and never join vertices in the same column</li>
</ol>
Note that the above two condidtions are both <img src="http://cs-people.bu.edu/behoppe/videos/three-color.jpg" alt="Non-bipartite graph" hspace="20" vspace="20" border="2" align="right" title="Non-bipartite graph" />saying the same thing. For example, the  graph below is non-bipartite:<br />
<br />
Suppose we color the first node red; then we must color the second node blue. But then there is no way to color the third node red or blue without creating either a red-red edge or a blue-blue edge.<br />
<h2><a name="bookmarks" id="bookmarks"></a>Social bookmarking, bipartite graphs, and structural equivalence</h2>
<p>One application of bipartite networks is the organization of user-created tags, such as the tags in <a href="http://del.icio.us/tags">del.icio.us</a>. Together with the website URLs they are associated with, these tags form a bipartite network. For example, the yellow nodes below left are tags on del.icio.us, and the orange nodes below right are some of the URLs associated with these tags:<br />
</p>
<div align="center"><img src="images/bipartite tags.jpg" alt="Bipartite Tags" width="583" height="480" hspace="20" vspace="20" border="2" align="middle" /></div>
<p class="style6">Even a machine that doesn't understand the tag-words themselves can analyze how the tag-words relate to each other based on how others associate those tags with website URLs. <br />
</p>
<p class="style6">The simplest indicator of tag relatedness is the group interlock network, which is explained in <em>Six Degrees</em>. Unfortunately, the group interlock network is too crude for many purposes, because it provides only a yes/no indicator and does not indicate relative amount of relatedness.</p>
<p class="style6">A more useful measurement of tag (or node) relatedness is based on <a href="#struct">structural equivalence</a>, which we defined previously.</p>
<p class="style6">The structural equivalence of two tags is 1 when they have identical website associations and 0 when two tags have not a single common website associated with them. <br />
</p>
<p>Examples based on the above yellow &amp; orange graph: </p>
<p>The structural equivalence of YouTube and Video is</p>
<p>|N(YouTube) ∩ N(Video)| / |N(YouTube) ∪ N(Video)|</p>
<p>=   |{A, B, C, F} ∩ {A, B, C, D, E, F}| / |{A, B, C, F} ∪  {A, B, C, D, E, F}|</p>
<p>= |{A, B, C, F}| / |{A, B, C, D, E, F}|</p>
<p>=  4 / 6</p>
<p>= 2/3<span class="style6"><br />
</span></p>
<p class="style6">&nbsp;</p>
<p class="style6">The structural equivalence of MySpace and Video is</p>
<p class="style6">|N(MySpace) ∩ N(Video)| / |N(MySpace) ∪ N(Video)|</p>
<p class="style6">=|{H, I, K} ∩ {A, B, C, D, E, F}| / |{H, I, K}  ∪ {A, B, C, D, E, F}|</p>
<p class="style6">=|{ }| / |{A, B,C, D, E, F, H, I, K}|</p>
<p class="style6">= 0 / 9</p>
<p class="style6">= 0<br />
</p>
[stop]
<hr/>
<h2><a name="trees" id="trees">Trees</a> </h2>
<p>A <strong>tree</strong> is a graph in which any two <a title="Vertex" href="http://en.wikipedia.org/wiki/Vertex">vertices</a> are connected by <em>exactly one</em> path. <br />
  <br />
Here are three different trees made from the same set of nodes:<br />
<img src="http://cs-people.bu.edu/behoppe/videos/trees.jpg" alt="trees" title="trees" border="0" hspace="0" vspace="0" /> <br />
</p>
<!-- start content -->
<h3>Rooted (or Hierarchical) Trees</h3>
<p>In computer science, we often use trees for hierarchical organization of data, in which case we use rooted trees.<br />
    <img src="http://cs-people.bu.edu/behoppe/videos/rooted%20tree.jpg" alt="rooted tree" title="rooted tree" border="0" hspace="0" vspace="0" /><br />
  In the above tree, node 3 is specially designated as the root of the tree. Every other node in the tree is implicitly ranked by its distance from the root. <br />
</p>
<p>For any edge in a rooted tree, one end of the edge must be closer to the root than the other. We call the node closer to the root the parent of the other node, and we call the node farther from the root a child of its parent node. For example, nodes 9 and 4 are children of node 3. Similarly, nodes 10, 6, and 5 are all children of node 4. (And node 2 is the only child of node 9.) Any node in a rooted tree may have any number of children (including zero). A node with no children is called a leaf (examples leaf nodes are 1, 10, 7, and 5). Every node except the root node must have exactly one parent. For example, the parent of node 7 is node 6. Note that the above rooted tree is undirected, but each edge in the rooted tree has an implicit direction based on the parent-child relationship represented by that edge.<br />
</p>
<p>Any node of a tree can be a valid root of that tree. For example, if we decided that node 6 was actually the central anchor of the above network, we could re-root the exact same tree at 6 instead of 3:<br />
</p>
<p><img src="http://cs-people.bu.edu/behoppe/videos/rerooted%20tree.jpg" alt="rerooted tree" title="rerooted tree" border="0" hspace="0" vspace="0" /><br />
  Check and see that the two graphs above have exactly the same sets of nodes and edges. What happens to parent-child relationships when a tree is re-rooted?<br />
</p>
<p>In terms of its graph theoretic properties, a rooted tree is equivalent to the traditional hierarchical organizational chart:<br />
    <img src="http://upload.wikimedia.org/wikipedia/en/d/dc/Organizational_chart.jpg" alt="Org Chart" title="Org Chart" border="2" hspace="0" vspace="0" /><br />
  The Division Officer at the top of the above hierarchy would probably argue that changing the root of the corresponding tree is a bad idea. However, if we are using trees to connect computers or concepts rather than people, we are less likely to encounter resistance when considering different alternatives for designating the root of a tree.<br />
</p>
<h3>Important properties of trees</h3>
A tree (rooted or not) may also be defined by the following equivalent conditions: <br />
<ol>
  <li>A <a title="Connected graph" href="http://en.wikipedia.org/wiki/Connected_graph">connected</a> graph with no <a title="Cycle (graph theory)" href="http://en.wikipedia.org/wiki/Cycle_%28graph_theory%29">cycles</a>.</li>
  <li>A connected graph with <em>|E| = |V| - 1</em></li>
</ol>
Definition #1 above gives us an easy way to eyeball a graph and see if it is a tree. Does the graph consist of one connected component or does it fall apart into several connected components? A tree must be connected. Does the graph have any cycles (intuitively, a path that ends where it started)? A tree cannot have any cycles.<br />
<br />
Which of the following three graphs are trees or not trees?<br />
<img src="http://cs-people.bu.edu/behoppe/videos/not%20trees%20and%20tree.jpg" alt="Not Trees and tree" title="Not Trees and tree" border="0" hspace="0" vspace="0" /><br />
<br />
Definition #2 above gives us a precise relationship between the number of nodes and edges in a tree, which can be very useful when designing algorithms that involve trees.<br />
<br />
It is somewhat remarkable that these three conditions are completely equivalent:<br />
<ol>
  <li>G is a <a title="Connected graph" href="http://en.wikipedia.org/wiki/Connected_graph">connected</a> graph with no <a title="Cycle (graph theory)" href="http://en.wikipedia.org/wiki/Cycle_%28graph_theory%29">cycles</a>.</li>
  <li>G is a connected graph with <em>|E| = |V| - 1.</em></li>
  <li>G is a graph in which any two vertices are connected by exactly one path.</li>
</ol>
To understand better why these three are in fact equivalent, we will use mathematical induction.<br />
<br />
<table border="4" width="100%">
  <tbody>
    <tr>
      <td valign="top" width="100%"><h3>Mathematical Induction</h3>
        When you write a computer program, how do you know it will work? A <a href="http://en.wikipedia.org/wiki/Mathematical_proof">mathematical proof</a> is a rigorous explanation of such a claim. (Example claim: &quot;MapQuest finds the shortest path between any two addresses in the United States.&quot;) A proof starts with statements that are accepted as true (called axioms) and uses formal logical arguments to show that the desired claim is a necessary consequence of the accepted statements. <br />
        <br />
        <a href="http://en.wikipedia.org/wiki/Mathematical_induction">Mathematical induction</a> (or simply induction) is one of the most important proof techniques in all of computer science. Induction is a systematic way of building from small example cases of a claim to show that the claim is true in general for all possible examples.<br />
        <br />
        Every proof by induction has two parts:<br />
        <ol>
          <li>Base case: Look at the smallest possible example case of the desired claim and show that the claim is true. The base case is often disarmingly trivial.</li>
          <li>Inductive step: Here we get to assume the inductive hypothesis: that the claim holds for all example case of size i or less. Given that assumption, we must prove that the claim also holds for all example cases of size i+1.</li>
        </ol>
        That's it! Here's an example proof by induction. We will prove something we discussed in class weeks ago:<br />
        <br />
        Suppose we want to prove that the maximum possible number of edges in an undirected graph with n nodes is n*(n-1)/2. Let's prove this by induction:<br />
        <ol>
          <li> Base case: Consider a graph with one node. This graph can have no more than 0 edges. Checking our claimed formula when n=1, we verify 1*(1-1)/2 = 0.</li>
          <li> Inductive step: The inductive hypothesis lets us assume that any graph with i nodes can have i*(i-1)/2 edges but no more. Now we have a graph with n=i+1 nodes. What is the maximum possible number of edges in this graph? We hope to show that this is (i+1)*((i+1)-1)/2 = (i+1)*i/2.</li>
        </ol>
        We take advantage of our inductive hypothesis by aribitrarily numbering the nodes of our graph 1, 2, 3, ..., i, i+1. Then we consider the first i nodes as one part of our graph and node i+1 as the other part of our graph:<br />
        <div><img src="http://cs-people.bu.edu/behoppe/videos/maxedge%20induction.jpg" alt="Max Edge Induction" title="Max Edge Induction" border="0" hspace="0" vspace="0" /><br />
        </div>
        To count the maximum possible edges in our graph, we consider <br />
        <ol>
          <li>The maximum possible number of edges in the red circle (nodes 1, 2, 3, ..., i)</li>
          <li>Plus the maximum possible number of edges outside the red circle--those edges incident to node i+1</li>
        </ol>
        The inductive hypothesis tells us the maximum possible number of edges in the red circle is i*(i-1)/2. <br />
        <br />
        The maximum number of edges incident to node i+1 is simply i (which would happen if node i+1 is adjacent to every other node in the graph).<br />
        <br />
        Adding the above two terms gives us the maximum number of edges in a graph with n=i+1 nodes equal to<br />
        <table border="0" width="100%">
          <tbody>
            <tr>
              <td valign="top" width="14%"><br />              </td>
              <td valign="top" width="14%"><br />              </td>
              <td valign="top" width="14%">i*(i-1)<br />
                ------- + i <br />
                2 </td>
              <td><br />
                = <br />              </td>
              <td valign="top" width="14%">i2 - i + 2i<br />
                ----------- <br />
                2 </td>
              <td><br />
                = <br />              </td>
              <td valign="top" width="14%">(i+1)*i<br />
                --------<br />
                2 </td>
              <td valign="top" width="14%"><br />              </td>
              <td valign="top" width="14%"><br />              </td>
            </tr>
          </tbody>
        </table>
        <br />
        Voila! Check for yourself that we have completed both the base case and the inductive step. We now have a complete proof by induction that the maximum number of edges in a graph with n nodes is n*(n-1)/2. </td>
    </tr>
  </tbody>
</table>
<br />
Now we return to our original goal, to understand better why these three conditions are equivalent:<br />
<ol>
  <li>G is a <a title="Connected graph" href="http://en.wikipedia.org/wiki/Connected_graph">connected</a> graph with no <a title="Cycle (graph theory)" href="http://en.wikipedia.org/wiki/Cycle_%28graph_theory%29">cycles</a>.</li>
  <li>G is a connected graph with <em>|E| = |V| - 1.</em></li>
  <li>G is a graph in which any two vertices are connected by exactly one path.</li>
</ol>
We will use induction to prove that a connected graph with no cycles has |E| = |V| - 1.<br />
<ol>
  <li> Base Case: Consider a graph with one node. A one-node graph is trivially connected and has no cycles. It cannot have any edges. We immediately get |E| = |V| - 1 = 0.</li>
  <li> Inductive Step: Consider any connected graph with no cycles and no more than i nodes. The inductive hypothesis lets us assume that the number of edges is one less than the number of nodes for any such graph. Now consider a connected graph with no cycles and i+1 nodes. We aim to show that any such graph has exactly i edges, or that |E| = |V| - 1 = (i+1)-1 = i.</li>
</ol>
We take advantage of the inductive hypothesis by aribitrarily numbering the nodes of our graph 1, 2, 3, ..., i, i+1. Then we consider the first i nodes as one part of the graph and node i+1 as the other part of the graph.<br />
<br />
Because the graph is connected, there is at least one edge from node i+1 to the rest of the graph. There may be more than one edge from node i+1 to the rest of the graph. Below we draw the case where there are three such edges:<br />
<div><img src="http://cs-people.bu.edu/behoppe/videos/edges%20per%20tree%20induction.jpg" alt="Edges Per Tree Induction" title="Edges Per Tree Induction" border="0" hspace="0" vspace="0" /><br />
</div>
Here is the crux of the inductive step: If we remove node i+1 from the graph, then the rest of the graph breaks up into sub-trees. The number of sub-trees is exactly equal to the number of edges incident to node i+1.<br />
<br />
To illustrate why this is so, we have colored the edges incident to node i+1 in the above example along with the sub-trees they connect to. Notice how each of the three &quot;NO&quot; scenarios violates one of our assumptions, either by creating a cycle (which either of the two red dashed edges would do) or by creating a disconnected graph (which the grey node would do).<br />
<br />
Re-iterating: However many edges are incident to node i+1, then the nodes 1, 2, 3, ..., i break up into exactly that many sub-trees when we remove node i+1 and its edges.<br />
<br />
Because each of the sub-trees has no more than i nodes, we can use the inductive hypothesis to tell us that each sub-tree has exactly one less edge than it has nodes.<br />
<br />
In the above example, this means<br />
<table border="0">
  <tbody>
    <tr>
      <td valign="top" width="25%"><div># edges in blue sub-tree = <br />
        # edges in green sub-tree = <br />
        # edges in yellow sub-tree = <br />
      </div></td>
      <td valign="top" width="25%">|Vblue| - 1<br />
        |Vgreen| - 1<br />
        |Vyellow| - 1 </td>
    </tr>
  </tbody>
</table>
<br />
Although we don't know how many nodes are in each sub-tree, we do know the total number of nodes in all the sub-trees is i. So if we add the three equalities above, we get<br />
<br />
<table border="0">
  <tbody>
    <tr>
      <td valign="top" width="25%"><div># edges in all sub-trees =<br />
      </div></td>
      <td valign="top" width="25%">i - 3<br />      </td>
    </tr>
  </tbody>
</table>
Now we add the three edges incident to node i+1 (three--as drawn in the example above) to get<br />
<br />
<table border="0">
  <tbody>
    <tr>
      <td valign="top" width="25%"><div>|E| =<br />
      </div></td>
      <td valign="top" width="25%">i - 3 + 3 = i<br />      </td>
    </tr>
  </tbody>
</table>
<br />
Three is a number we arbitrarily chose for the sake of this example--and notice how the 3's cancel each other out in the above sum. No matter whether there are 3, 2, 1, or whatever number of edges incident to node i+1, when we look at a connected graph with no cycles and i+1 nodes, we will end up with |E| = i, just as we aimed to prove. This completes our proof by induction that a connected graph with no cycles has |E| = |V| - 1.<br />
<h2>Minimum cost trees</h2>
Suppose your new office building is being wired for Internet. For the service to be worth much, your network must be connected (a path between every pair of computers); however, you probably don't want to install direct cables connecting every possible pair of computers.<br />
<br />
Trees provide a useful starting point for any network service provider facing this sort of problem. This is because a tree connects a network with the minimum possible number of edges. (Think about why this is true.)<br />
<br />
Simply finding any tree to connect a network isn't enough though. Not all computers (or nodes) are equally costly to join directly. Consider the network below:<br />
<a href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_0.svg"><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Kruskal_Algorithm_0.svg/200px-Kruskal_Algorithm_0.svg.png" alt="K1" title="K1" border="0" hspace="0" vspace="0" /></a><br />
There are numbers next to each edge. Each number represents the cost (or weight) of the edge. In the above example, the weight of each edge corresponds to how far apart its endpoints are.<br />
<br />
Suppose we want to choose a subset of edges to connect the above 7 nodes in a tree. Any tree connecting 7 nodes must have exactly 6 edges, so finding a &quot;minimum edge tree&quot; is meaningless. What we want are the 6 edges that form a tree for the least total cost.<br />
<br />
We call this problem the minimum cost tree problem. It is often called the minimum spanning tree problem--&quot;spanning&quot; being a term applying to any subgraph that includes every possible node of the original graph.<br />
<br />
The minimum cost tree problem turns out to one that is both remarkably important in real life and also easy to compute. Below is an algorithm that starts with a graph G = (V,E) and computes a set of edges T that form a tree (V,T) that has minimum possible total cost:<br />
<ol>
  <li>create a set T, initially equal to { }</li>
  <li>create a set F, initially equal to E</li>
  <li>if F = { } then go to step 7<br />
  </li>
  <li>remove an edge {x,y} with minimum cost from F<br />
  </li>
  <li>if x and y are already connected by a path of edges in T, then discard edge {x,y}; otherwise add edge {x,y} to T</li>
  <li>go to step 3<br />
  </li>
  <li>done--the minimum cost tree is (V,T)</li>
</ol>
The above algorithm is called Kruskal's algorithm.<br />
<h3>Example</h3>
<p>We will apply Kruskal's algorithm to the graph above. Recall that set F initially includes every edge. Set T is initially empty. As we add edges to T in the example below, we color those edges green.<br />
</p>
<table border="1" cellpadding="0" cellspacing="0">
  <tbody>
    <tr>
      <td><p><a href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_0.svg">
        <!--[if gte vml 1]><v:shapetype    id="_x0000_t75" coordsize="21600,21600" o:spt="75" o:preferrelative="t"    path="m@4@5l@4@11@9@11@9@5xe" filled="f" stroked="f">    <v:stroke joinstyle="miter"/>    <v:formulas>     <v:f eqn="if lineDrawn pixelLineWidth 0"/>     <v:f eqn="sum @0 1 0"/>     <v:f eqn="sum 0 0 @1"/>     <v:f eqn="prod @2 1 2"/>     <v:f eqn="prod @3 21600 pixelWidth"/>     <v:f eqn="prod @3 21600 pixelHeight"/>     <v:f eqn="sum @0 0 1"/>     <v:f eqn="prod @6 1 2"/>     <v:f eqn="prod @7 21600 pixelWidth"/>     <v:f eqn="sum @8 21600 0"/>     <v:f eqn="prod @7 21600 pixelHeight"/>     <v:f eqn="sum @10 21600 0"/>    </v:formulas>    <v:path o:extrusionok="f" gradientshapeok="t" o:connecttype="rect"/>    <o:lock v:ext="edit" aspectratio="t"/>   </v:shapetype><v:shape id="_x0000_i1025" type="#_x0000_t75" alt=""    href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_0.svg" title="&quot;&quot;"    style='width:150pt;height:126pt' o:button="t">    <v:imagedata src="file:///C:\DOCUME~1\Bruce\LOCALS~1\Temp\msohtml1\01\clip_image001.png"     o:href="http://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Kruskal_Algorithm_0.svg/200px-Kruskal_Algorithm_0.svg.png"/>   </v:shape><![endif]-->
        <!--[if !vml]-->
        <img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Kruskal_Algorithm_0.svg/200px-Kruskal_Algorithm_0.svg.png" alt="K1" title="K1" border="0" hspace="0" vspace="0" /><br />
        <!--[endif]-->
      </a></p></td>
      <td><p>This is our original graph. The numbers near the arcs indicate their weight. None of the arcs are highlighted.</p></td>
    </tr>
    <tr>
      <td><p><a href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_1.svg">
        <!--[if gte vml 1]><v:shape    id="_x0000_i1026" type="#_x0000_t75" alt=""    href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_1.svg" title="&quot;&quot;"    style='width:150pt;height:126pt' o:button="t">    <v:imagedata src="file:///C:\DOCUME~1\Bruce\LOCALS~1\Temp\msohtml1\01\clip_image003.png"     o:href="http://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Kruskal_Algorithm_1.svg/200px-Kruskal_Algorithm_1.svg.png"/>   </v:shape><![endif]-->
        <!--[if !vml]-->
        <img src="http://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Kruskal_Algorithm_1.svg/200px-Kruskal_Algorithm_1.svg.png" alt="K2" title="K2" border="0" hspace="0" vspace="0" /><br />
        <!--[endif]-->
      </a></p></td>
      <td><p><strong>{A,D}</strong> and <strong>{C,E}</strong> are the shortest arcs, with length 5, and <strong>{A,D}</strong> has been arbitrarily chosen, so it is highlighted.</p></td>
    </tr>
    <tr>
      <td><p><a href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_2.svg">
        <!--[if gte vml 1]><v:shape    id="_x0000_i1027" type="#_x0000_t75" alt=""    href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_2.svg" title="&quot;&quot;"    style='width:150pt;height:126pt' o:button="t">    <v:imagedata src="file:///C:\DOCUME~1\Bruce\LOCALS~1\Temp\msohtml1\01\clip_image005.png"     o:href="http://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Kruskal_Algorithm_2.svg/200px-Kruskal_Algorithm_2.svg.png"/>   </v:shape><![endif]-->
        <!--[if !vml]-->
        <img src="http://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Kruskal_Algorithm_2.svg/200px-Kruskal_Algorithm_2.svg.png" alt="K3" title="K3" border="0" hspace="0" vspace="0" /><br />
        <!--[endif]-->
      </a></p></td>
      <td><p>However, <strong>{C,E}</strong> is now the shortest arc that does not form a cycle, with length 5, so it is highlighted as the second arc.</p></td>
    </tr>
    <tr>
      <td><p><a href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_3.svg">
        <!--[if gte vml 1]><v:shape    id="_x0000_i1028" type="#_x0000_t75" alt=""    href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_3.svg" title="&quot;&quot;"    style='width:150pt;height:126pt' o:button="t">    <v:imagedata src="file:///C:\DOCUME~1\Bruce\LOCALS~1\Temp\msohtml1\01\clip_image007.png"     o:href="http://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Kruskal_Algorithm_3.svg/200px-Kruskal_Algorithm_3.svg.png"/>   </v:shape><![endif]-->
        <!--[if !vml]-->
        <img src="http://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Kruskal_Algorithm_3.svg/200px-Kruskal_Algorithm_3.svg.png" alt="K4" title="K4" border="0" hspace="0" vspace="0" /><br />
        <!--[endif]-->
      </a></p></td>
      <td><p>The next arc, <strong>{D,F}</strong> with length 6, is highlighted using much the same method.</p></td>
    </tr>
    <tr>
      <td><p><a href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_4.svg">
        <!--[if gte vml 1]><v:shape    id="_x0000_i1029" type="#_x0000_t75" alt=""    href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_4.svg" title="&quot;&quot;"    style='width:150pt;height:126pt' o:button="t">    <v:imagedata src="file:///C:\DOCUME~1\Bruce\LOCALS~1\Temp\msohtml1\01\clip_image009.png"     o:href="http://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Kruskal_Algorithm_4.svg/200px-Kruskal_Algorithm_4.svg.png"/>   </v:shape><![endif]-->
        <!--[if !vml]-->
        <img src="http://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Kruskal_Algorithm_4.svg/200px-Kruskal_Algorithm_4.svg.png" alt="K5" title="K5" border="0" hspace="0" vspace="0" /><br />
        <!--[endif]-->
      </a></p></td>
      <td><p>The next-shortest arcs are <strong>{A,B}</strong> and <strong>{B,E}</strong>, both with length 7. <strong>{A,B}</strong> is chosen arbitrarily, and is highlighted. The arc <strong>{B,D}</strong> has been highlighted in red, because it would form a cycle <strong>(A,B,D,A)</strong> if it were chosen.</p></td>
    </tr>
    <tr>
      <td><p><a href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_5.svg">
        <!--[if gte vml 1]><v:shape    id="_x0000_i1030" type="#_x0000_t75" alt=""    href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_5.svg" title="&quot;&quot;"    style='width:150pt;height:126pt' o:button="t">    <v:imagedata src="file:///C:\DOCUME~1\Bruce\LOCALS~1\Temp\msohtml1\01\clip_image011.png"     o:href="http://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Kruskal_Algorithm_5.svg/200px-Kruskal_Algorithm_5.svg.png"/>   </v:shape><![endif]-->
        <!--[if !vml]-->
        <img src="http://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Kruskal_Algorithm_5.svg/200px-Kruskal_Algorithm_5.svg.png" alt="K6" title="K6" border="0" hspace="0" vspace="0" /><br />
        <!--[endif]-->
      </a></p></td>
      <td><p>The process continutes to highlight the next-smallest arc, <strong>{B,E}</strong> with length 7. Many more arcs are highlighted in red at this stage:<br />
      </p>
          <ul>
            <li><strong>{B,C}</strong> because it would form the cycle <strong>(B,C,E</strong>,B)</li>
            <li> <strong>{D,E}</strong> because it would form the cycle <strong>(D,E,B,A,D)</strong></li>
            <li><strong>{F,E}</strong> because it would form cycle <strong>(F,E,B,A,D,F)</strong>.</li>
          </ul>
        <p> </p></td>
    </tr>
    <tr>
      <td><p><a href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_6.svg">
        <!--[if gte vml 1]><v:shape    id="_x0000_i1031" type="#_x0000_t75" alt=""    href="http://en.wikipedia.org/wiki/Image:Kruskal_Algorithm_6.svg" title="&quot;&quot;"    style='width:150pt;height:126pt' o:button="t">    <v:imagedata src="file:///C:\DOCUME~1\Bruce\LOCALS~1\Temp\msohtml1\01\clip_image013.png"     o:href="http://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Kruskal_Algorithm_6.svg/200px-Kruskal_Algorithm_6.svg.png"/>   </v:shape><![endif]-->
        <!--[if !vml]-->
        <img src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Kruskal_Algorithm_6.svg/200px-Kruskal_Algorithm_6.svg.png" alt="K7" title="K7" border="0" hspace="0" vspace="0" /><br />
        <!--[endif]-->
      </a></p></td>
      <td><p>Finally, the process finishes with the arc <strong>{E,G}</strong> of length 9, and the minimum spanning tree is found.</p></td>
    </tr>
  </tbody>
</table>
<br />
<h2> <a name="pagerank" id="pagerank"></a>PageRank</h2>
<p>PageRank is the algorithm behind Google. According to Tim Berners-Lee, PageRank and its kin &quot;form the heart of the critical crawlers and rank-assessment algorithms behind Web search.&quot; </p>
<p><a href="http://en.wikipedia.org/wiki/PageRank#Description">Google describes PageRank</a>:</p>
<blockquote>
  <p> PageRank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page's value. In essence, Google interprets a link from page A to page B as a vote, by page A, for page B. But, Google looks at more than the sheer volume of votes, or links a page receives; it also analyzes the page that casts the vote. Votes cast by pages that are themselves &quot;important&quot; weigh more heavily and help to make other pages &quot;important&quot;.</p>
</blockquote>
<p>In other words, a PageRank results from a &quot;ballot&quot; among all the other pages on the World Wide Web about how important a page is. A hyperlink to a page counts as a vote of support. But, as mentioned above, not all votes count equally.</p>
<h3> Centrality</h3>
<p>The core network mathematical concept in PageRank is the notion of <a href="http://en.wikipedia.org/wiki/Centrality#eigenvector_centrality">centrality</a>. This is an intuitive notion that is surprisingly hard to pin down mathematically. For example, which node is the most central in the following graph?</p>
<p align="center"><img src="images/google centrality.jpg" alt="Google Centrality" hspace="20" vspace="20" border="2" align="middle" /></p>
<p>There is no one right answer to the question, because the question is too vague. There are many different ways to define centrality more precisely. Here are three important ones, all of which rely critically on the direction of links (something we have largely glossed over until now):</p>
<ul>
  <li><strong>Outdegree Centrality</strong>: Just another way of saying outdegree; &quot;Outdegree centrality&quot; defines centrality as a measure of how many links point out of a node. A webpage that has many clickable links on the screen has high outdegree centrality. A webpage with very few clickable links has low outdegree centrality. In the above graph, node 3 has the highest outdegree centrality; it is the only node with 3 outgoing links.</li>
</ul>
<ul>
  <li><strong>Indegree Centrality</strong>: Just another way of saying indegree: &quot;Indegree centrality&quot; defines centrality as a measure of how many links point into a node. A webpage that is linked to by many other webpages has high indegree centrality. A webpage that is not linked to by many other webpages has low indegree centrality. Node 6 above has the highest indegree centrality; it is the only node with 4 incoming links. Indegree centrality corresponds roughly to the popularity  of a website.<br />
</li>
</ul>
<ul>
  <li><strong>PageRank </strong>is a more sophisticated version of indegree centrality. The key refinement is that not every incoming link counts equally in computing PageRank--links from more popular neighbors count more than links from less popular neighbors. For example, notice that nodes 2, 3, and 5 all have equal indegree centrality (which is 1). However, node 5 is recommended by node 4, which is a very popular node. Nodes 2 and 3 are not popular and only receive recommendations from each other. So node 5 has significantly higher PageRank than nodes 2 and 3. </li>
</ul>
<p>The PageRank formula is described and illustrated with examples and an interactive PageRank calculator at <a href="http://www.webworkshop.net/pagerank.html">http://www.webworkshop.net/pagerank.html</a>. You can also find a rigorous mathematical introduction in <a href="http://en.wikipedia.org/wiki/Centrality#Eigenvector_centrality">Wikipedia</a> (not for the faint of heart).</p>
<p>WebWorkshop writes the PageRank formula in a manner that is compact but subtle. We restate the formula here and then introduce an easier, simplified forumulation. We begin by defining a  <strong>damping factor</strong>  d to be a fixed coefficient at least 0 but no more than 1. (Google uses d=0.85.) Then for any graph, we define the <strong>PageRank</strong> of each of its nodes as follows:</p>
<table border="2" align="center" cellpadding="4" cellspacing="4">
  <tr>
    <td bgcolor="#FFFFCC"><p>For any node x, let </p>
      <ul>
        <li>{y1, y2, ..., yn} denote the set of all nodes with directed links to x, </li>
      </ul>
      <p>Then</p>
    <p>PageRank(x) =</p>
    <blockquote>
      <p>(1-d) + d*<span class="style9">(</span>PageRank(y1)/Outdegree(y1) </p>
      <blockquote>
        <blockquote>
          <p>+PageRank(y2)/Outdegree(y2) </p>
          <p>... </p>
          <p>+ PageRank(yn)/Outdegree(yn)<span class="style9">)</span></p>
        </blockquote>
      </blockquote>
    </blockquote></td>
  </tr>
</table>
<p>Computing PageRank  from the above definition presents both major and minor difficulties:</p>
<ul>
  <li><strong>Major difficulty:</strong> The PageRank formula is defined in terms of itself -- notice that &quot;PageRank&quot; occurs in a non-trivial way on both sides of the formula. Any function that is defined in terms of itself is said to be a recursive function; understanding <a href="http://en.wikipedia.org/wiki/Recursion">recursion</a> is a deep theme underlying the entire field of computer science. Here we gloss over that and simply recognize that we have  a &quot;Catch-22&quot; situation: How can we calculate PageRank when the formula  above requires that we already know the value of  PageRank? We will manage this Catch-22 by starting with a very crude estimate of PageRank, and then running repeated iterations of calculations, which produce ever-more-accurate results by building on results of previous iterations.</li>
  <li><strong>Minor difficulty:</strong> Even once we accept the iterative approach to PageRank, the calculation still involves multiplying by a damping coefficient and dividing by node-outdegree, resulting in much messier fractions than we like to present in this course.</li>
</ul>
<p>Another problem with the above definition of PageRank is understanding the damping factor, which is rather obtuse. Here is the best way we know to explain the damping factor d:</p>
<div align="center">
<table width="0%" border="2" cellspacing="5" cellpadding="5">
  <tr>
    <td bgcolor="#FFFFCC"><p>One way to consider PageRank is as a kind of probability distribution. Suppose a Web surfer starts at a random page and clicks at random for a while (i.e., clicking on any link that happens to be on the current page, etc.). Occasionally the Web surfer types in a new  URL (randomly chosen from <em>all</em> Web pages) instead of link-clicking. Eventually the surfer stops at his &quot;destination&quot; page. The PageRank of a Web page <em>x</em> is essentially the probability that <em>x</em> will be the destination page of that surfer's random walk of clicking and URL-typing. In this scenario, the damping factor  d is essentially the probability that the &quot;random Web surfer&quot; clicks a  link instead of typing in a new URL. So... </p>
      <ul>
        <li>If d=1 then the resulting PageRank function assumes that the &quot;random Web surfer&quot; starts at a random page and then <em>always</em> clicks and <em>never</em> types in a new URL. </li>
        <li>If d=0 then the resulting PageRank function assumes that the &quot;random Web surfer&quot; <em>never</em> clicks; he <em>always</em> types in another random URL. (Admittedly the d=0 model corresponds to an unrealistic and/or pathological version of Web surfing; hence Google uses d=0.85, which is closer to 1 than 0.)</li>
    </ul></td>
  </tr>
</table></div>
<p>Here we present<strong> HopRank</strong> as a simpler version of PageRank. The HopRank function takes two inputs: a Web page x and a positive integer i. It is then written HopRank<sub>i</sub>(x). There are three  good ways to interpret HopRank<sub>i</sub>(x):</p>
<ol>
  <li>HopRank<sub>i</sub>(x) is a a simple way to estimate PageRank. It is essentially equivalent to the PageRank formula referenced above with damping factor d=1 and with no dividing by node outdegrees.</li>
  <li>HopRank<sub>i</sub>(x) is  a more sophisticated way to measure the indegree of x. HopRank<sub>i</sub>(x) takes into account not just how many incoming links are incident to  x; it also accounts for the  ranks of where those  links are coming from. One incoming link from a high-ranking node can be more important than many incoming links from many low-ranking nodes.</li>
  <li>HopRank<sub>i</sub>(x) simplifies the &quot;random Web surfer&quot; model of PageRank in the following way: HopRank<sub>i</sub>(x) = the number of directed walks of length i that end at node x.</li>
</ol>
<p>Perhaps the easiest way to understand HopRank is not by interpreting it (as above), but by showing how it is calculated, which we do below. The simplest case of HopRank<sub>i</sub>(x) is when i=1:</p>
<p align="center">HopRank<sub>1</sub>(x) = InDegree(x)</p>
<p>The above equality is simply the definition of HopRank<sub>1</sub>(x). From that starting point, we will proceed iteratively and define HopRank<sub>2</sub> based on HopRank<sub>1</sub>,  then HopRank<sub>3</sub> based on HopRank<sub>2</sub>, and so forth.</p>
<p>Given a directed graph G=(V,E), we  lay the foundation of the HopRank calculation by defining the InNeighborhood of a node x ∈ V to be </p>
<table width="0%" border="0" align="center" cellpadding="2" cellspacing="2">
  <tr>
    <td><div align="right">InN(x) =</div></td>
    <td>the set of all nodes that have directed edges into x</td>
  </tr>
  <tr>
    <td><div align="right">=</div></td>
    <td>{y<strong>:</strong> (y,x) ∈ E}</td>
  </tr>
</table>
<p>Then HopRank<sub>1</sub>(x) = |InN(x)| = the indegree of x. Below we illustrate this using same the  6-node graph as above: </p>
<div align="center">
<table width="0%" border="0" cellspacing="10" cellpadding="10">
  <tr>
    <td><table width="200" border="2">
  <tbody>
    <tr>
      <td valign="top" width="13%">Node&nbsp;x<br />
      </td>
      <td valign="top">InN(x)<br />
      </td>
      <td valign="top">HopRank<sub>1</sub>(x)<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top">{4,6}<br />
      </td>
      <td valign="top">2<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">2<br />
      </td>
      <td valign="top">{3}<br />
      </td>
      <td valign="top">1<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">3<br />
      </td>
      <td valign="top">{2}<br />
      </td>
      <td valign="top">1<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">4<br />
      </td>
      <td valign="top">{3,5,6}<br />
      </td>
      <td valign="top">3<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">5<br />
      </td>
      <td valign="top">{4}<br />
      </td>
      <td valign="top">1<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">6<br />
      </td>
      <td valign="top">{1,2,3,5}<br />
      </td>
      <td valign="top">4<br />
      </td>
    </tr>
  </tbody>
</table></td>
    <td><img src="images/google centrality.jpg" alt="Google Centrality" width="260" border="2" align="right" /></td>
  </tr>
</table></div>
<p>HopRank<sub>1</sub> is a very local measure of node centrality. We can measure a slightly more system-wide sense of node centrality by defining HopRank<sub>2</sub> as follows: For any node x, HopRank<sub>2</sub> of x is the sum of the HopRank<sub>1</sub>'s of all nodes in InN(x).<br />
</p>
<table width="347" border="2" align="center">
  <tbody>
    <tr>
      <td valign="top" width="13%">Node&nbsp;x<br />
      </td>
      <td valign="top">InN(x)<br />
      </td>
      <td valign="top">HopRank<sub>1</sub>(x)<br />
      </td>
      <td valign="top">HopRank<sub>2</sub>(x)<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top">{4,6}<br />
      </td>
      <td valign="top">2<br />
      </td>
      <td valign="top">7 = 3+4<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">2<br />
      </td>
      <td valign="top">{3}<br />
      </td>
      <td valign="top">1<br />
      </td>
      <td valign="top">1 = 1<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">3<br />
      </td>
      <td valign="top">{2}<br />
      </td>
      <td valign="top">1<br />
      </td>
      <td valign="top">1 = 1<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">4<br />
      </td>
      <td valign="top">{3,5,6}<br />
      </td>
      <td valign="top">3<br />
      </td>
      <td valign="top">6 = 1+1+4<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">5<br />
      </td>
      <td valign="top">{4}<br />
      </td>
      <td valign="top">1<br />
      </td>
      <td valign="top">3 = 3<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">6<br />
      </td>
      <td valign="top">{1,2,3,5}<br />
      </td>
      <td valign="top">4<br />
      </td>
      <td valign="top">5 = 2+1+1+1<br />
      </td>
    </tr>
  </tbody>
</table>
<p> We can then repeat the above process to calculate HopRank<sub>3</sub> based on HopRank<sub>2</sub>, then HopRank<sub>4</sub> based on HopRank<sub>3</sub>, etc. Below is a table of five successive calculations of HopRank<sub>i</sub>, based on the same graph as above:</p>
<div align="center">
<table border="2">
  <tbody>
    <tr>
      <td valign="top" width="13%">Node x<br />
      </td>
      <td valign="top" width="13%">InN(x)<br />
      </td>
      <td valign="top" width="13%">HopRank<sub>1</sub>(x)<br />
      </td>
      <td valign="top" width="13%">HopRank<sub>2</sub>(x)<br />
      </td>
      <td valign="top" width="13%">HopRank<sub>3</sub>(x)<br />
      </td>
      <td valign="top" width="13%">HopRank<sub>4</sub>(x)<br />
      </td>
      <td valign="top" width="13%">HopRank<sub>5</sub>(x)<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top" width="13%">{4,6}<br />
      </td>
      <td valign="top" width="13%">2<br />
      </td>
      <td valign="top" width="13%">7<br />
      </td>
      <td valign="top" width="13%">11<br />
      </td>
      <td valign="top" width="13%">21<br />
      </td>
      <td valign="top" width="13%">38<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">2<br />
      </td>
      <td valign="top" width="13%">{3}<br />
      </td>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top" width="13%">1<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">3<br />
      </td>
      <td valign="top" width="13%">{2}<br />
      </td>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top" width="13%">1<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">4<br />
      </td>
      <td valign="top" width="13%">{3,5,6}<br />
      </td>
      <td valign="top" width="13%">3<br />
      </td>
      <td valign="top" width="13%">6<br />
      </td>
      <td valign="top" width="13%">9<br />
      </td>
      <td valign="top" width="13%">19<br />
      </td>
      <td valign="top" width="13%">29<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">5<br />
      </td>
      <td valign="top" width="13%">{4}<br />
      </td>
      <td valign="top" width="13%">1<br />
      </td>
      <td valign="top" width="13%">3<br />
      </td>
      <td valign="top" width="13%">6<br />
      </td>
      <td valign="top" width="13%">9<br />
      </td>
      <td valign="top" width="13%">19<br />
      </td>
    </tr>
    <tr>
      <td valign="top" width="13%">6<br />
      </td>
      <td valign="top" width="13%">{1,2,3,5}<br />
      </td>
      <td valign="top" width="13%">4<br />
      </td>
      <td valign="top" width="13%">5<br />
      </td>
      <td valign="top" width="13%">12<br />
      </td>
      <td valign="top" width="13%">19<br />
      </td>
      <td valign="top" width="13%">32<br />
      </td>
    </tr>
  </tbody>
</table></div>
<p>Below we redraw the same table, highlighting exactly how the row for node 4 is calculated. Because InN(4) = {3,5,6}, in each iteration i the calculation of HopRank<sub>i</sub>(4) adds the HopRank<sub>(i-1)</sub> of nodes 3, 5, and 6:</p>
<p align="center"><img src="images/inrank.png" alt="inrank" /></p>
<p>The above tabular calculation leads us to our most concise complete definition of HopRank<sub>i</sub>(x). Again, we assume we are given a directed graph G=(V,E), a node x∈V, and a positive integer i:</p>
<table width="0%" border="0" align="center" cellpadding="4" cellspacing="4">
  <tr>
    <td valign="middle">if i = 1:</td>
    <td valign="middle"><div align="right">HopRank<sub>1</sub>(x) =</div></td>
    <td colspan="2" valign="top"><div align="left"><span class="style12">|</span>InN(x)<span class="style12">|</span></div></td>
  </tr>
  <tr>
    <td align="left" valign="middle">if
      i &gt; 1:    </td>
    <td valign="middle"><div align="right">HopRank<sub>i</sub>(x) =</div></td>
    <td valign="top">
      <div align="center"><img width="30" src="images/sigma" alt="sigma" /><br/>
        <span class="style11">y∈InN(x)</span> </div></td>
    <td valign="middle"><span class="style12">(</span>HopRank<sub>(i-1)</sub>(y)<span class="style12">)</span></td>
  </tr>
</table>
<p>Looking back at the  tabular calculation, notice that relative rankings of  nodes  change from column to column in the table. Sometimes node 6 is ranked highest; sometimes node 1 is ranked highest. After &quot;enough&quot; iterations, the rank order of relative nodes will stabilize. Even more importantly, the relative ratios of HopRank<sub>i</sub> for different nodes will stabilize. </p>
<p>In other words, when the value of i is &quot;big enough,&quot; then the relative ratios of HopRank<sub>i</sub> for different nodes will not change in any significant way from iteration i to iteration i+1. According to <a href="http://www.webworkshop.net/pagerank.html">http://www.webworkshop.net/pagerank.html</a>, Google uses 40-50 iterations to calculate its version of PageRank. So we can informally say that HopRank<sub>50</sub> will give us a good simplified version of PageRank, where the relative rankings of nodes are stable.</p>
</body>
</html>
